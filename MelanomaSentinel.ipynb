{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Melanoma Staging Model\n",
    "\n",
    "The goal of this project is to create an easily understood and used tool for doctors to predict whether patients need a sentinel lymph node biopsy (SLNB). SLNB are an invasive procedure and currently doctors suggest it to patients if there is around a 10% chance for a positive biopsy. These odds are mostly based upon the depth of the melanoma, so I would like to use more data in order to create better predictions for the odds that patients need the biopsy. \n",
    "\n",
    "## The Data\n",
    "I am using data from two sources: the National Cancer Database and the NIH's SEER Database. Currently, we are soley focusing on the NCDB's melanoma patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import  psycopg2 as sql\n",
    "\n",
    "#mela = pd.read_csv('Melanoma.csv')\n",
    "sql_mela = sql.connect(host=\"localhost\", user=\"karenlarson\", dbname=\"melanoma\")\n",
    "#attribs = ['AGE', 'SEX', 'CS_SITESPECIFIC_FACTOR_1', 'CS_SITESPECIFIC_FACTOR_2',\n",
    " #          'CS_SITESPECIFIC_FACTOR_7', 'CS_EXTENSION', 'PRIMARY_SITE', 'CS_SITESPECIFIC_FACTOR_3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT \"AGE\", \"SEX\", \"DEPTH\", \"ULCERATION\", \"MITOSES\", \"CS_EXTENSION\", \"PRIMARY_SITE\", \"CS_LYMPH_NODE_METS\"\n",
    "            FROM melanoma\"\"\"\n",
    "#query = \"\"\"SELECT \"AGE\", \"SEX\", \"DEPTH\", \"ULCERATION\", \"MITOSES\", \"CS_EXTENSION\", \"PRIMARY_SITE\", \"CS_LYMPH_NODE_METS\"\n",
    "#           FROM melanoma\n",
    "#           WHERE (\"DEPTH\" <= 400 AND \"DEPTH\" >= 100)\n",
    "#           OR ((\"DEPTH\" < 100 AND \"DEPTH\" >= 75) AND\n",
    "#           (\"ULCERATION\" >= 1 OR \"MITOSES\" >= 1 OR \"AGE\" < 40 )) OR \n",
    "#           ((\"DEPTH\" < 75 AND \"DEPTH\" >= 54) AND \n",
    "#           ((\"MITOSES\" >= 1 AND \"ULCERATION\" >= 1) OR (\"ULCERATION\" >= 1\n",
    "#           AND \"AGE\" < 40) OR (\"MITOSES\" >= 1 AND \"AGE\" < 40)));\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela = pd.read_sql_query(query, sql_mela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 860205 entries, 0 to 860204\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   AGE                 860205 non-null  int64  \n",
      " 1   SEX                 860205 non-null  int64  \n",
      " 2   DEPTH               860201 non-null  float64\n",
      " 3   ULCERATION          860202 non-null  float64\n",
      " 4   MITOSES             854683 non-null  float64\n",
      " 5   CS_EXTENSION        860202 non-null  float64\n",
      " 6   PRIMARY_SITE        860205 non-null  object \n",
      " 7   CS_LYMPH_NODE_METS  860202 non-null  float64\n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 52.5+ MB\n"
     ]
    }
   ],
   "source": [
    "mela.info()\n",
    "data = mela.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have 523492 patients with 126 features (or attributes). Since it is a database, not all of these attributes would be known before a SLNB would be suggested and others of them would be irrelevant to the need for a biopsy. For example, <code>PUF_FACILITY_ID</code> (or the facility where these data were recorded) likely doesn't influence a patient's need for a biopsy. I will use the following subset of features below:\n",
    "\n",
    "- <code>AGE</code>: The age of the patient\n",
    "- <code>SEX</code>: The sex of the patient\n",
    "- <code>RACE</code>: The race of the patient\n",
    "- <code>SPANISH_HISPANIC_ORIGIN</code>: Indicates whether a patient is hispanic\n",
    "- <code>CS_SITESPECIFIC_FACTOR_1</code>: The depth of the tumor. I rename this feature <code>DEPTH</code> to be more easily interpable\n",
    "- <code>CS_SITESPECIFIC_FACTOR_2</code>: The formation of an ulcer on the skin. I rename this feature <code>ULCERATION</code> to be more easily interpable\n",
    "- <code>CS_EXTENSION</code>: Indicates the extent of the spread of the melanoma.\n",
    "- <code>CS_SITESPECIFIC_FACTOR_7</code>: Categories for the number of mitoses per square mm around a \"hot spot\". I rename this feature to <code>MITOSES</code> for interprability.\n",
    "- <code>PRIMARY_SITE</code>: The main site for the melanoma. The options are:\n",
    "    - <code>C44.0</code>: Skin of lips, NOS\n",
    "    - <code>C44.1</code>: Eyelid\n",
    "    - <code>C44.2</code>: External Ear\n",
    "    - <code>C44.3</code>: Skin of other and unspecified parts of face\n",
    "    - <code>C44.4</code>: Skin of scalp and neck\n",
    "    - <code>C44.5</code>: Skin of the trunk\n",
    "    - <code>C44.6</code>: Skin of upper limb and shoulder\n",
    "    - <code>C44.7</code>: Skin of lower limb and hip\n",
    "    - <code>C44.8</code>: Overlapping lesion of skin\n",
    "    - <code>C44.9</code>: Skin, NOS\n",
    " \n",
    "We are trying to calssify the patients into whether they had a positive lymph node biopsy or not. This is recorded in <code>TNM_PATH_N</code> or <code>REGIONAL_NODES_POSITIVE</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mela['TNM_PATH_N'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Combining\n",
    "\n",
    "Some of the rows of the features are missing data. To start with, I will drop rows with missing data in order to have the best possible training data possible. This is a reasonable assumption currently because I have a large amount of training data, but I can work on creating a model including interpolation at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attribs = ['AGE', 'SEX', 'CS_SITESPECIFIC_FACTOR_1', 'CS_SITESPECIFIC_FACTOR_2',\n",
    "     #      'CS_SITESPECIFIC_FACTOR_7', 'CS_EXTENSION', 'PRIMARY_SITE', 'CS_SITESPECIFIC_FACTOR_3']\n",
    "#num_attribs = ['AGE', 'SEX', 'CS_SITESPECIFIC_FACTOR_1', 'CS_SITESPECIFIC_FACTOR_2',\n",
    "      #     'CS_SITESPECIFIC_FACTOR_7', 'CS_EXTENSION', 'CS_SITESPECIFIC_FACTOR_3']\n",
    "#attribs = ['AGE', 'SEX', 'RACE', 'SPANISH_HISPANIC_ORIGIN', 'CS_SITESPECIFIC_FACTOR_1', 'CS_SITESPECIFIC_FACTOR_2',\n",
    "#           'CS_SITESPECIFIC_FACTOR_7', 'CS_EXTENSION', 'PRIMARY_SITE', 'REGIONAL_NODES_POSITIVE']\n",
    "#attribs = ['AGE', 'SEX', 'RACE', 'SPANISH_HISPANIC_ORIGIN', 'CS_SITESPECIFIC_FACTOR_1', 'CS_SITESPECIFIC_FACTOR_2',\n",
    "#           'CS_SITESPECIFIC_FACTOR_7', 'CS_EXTENSION', 'PRIMARY_SITE', 'TNM_PATH_N']\n",
    "\n",
    "#attribs = ['AGE', 'SEX', 'CS_SITESPECIFIC_FACTOR_1', 'CS_SITESPECIFIC_FACTOR_2',\n",
    "#           'CS_SITESPECIFIC_FACTOR_7', 'CS_EXTENSION', 'PRIMARY_SITE', 'TNM_PATH_N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = mela[attribs].dropna()\n",
    "#data[num_attribs] = data[num_attribs].apply(pd.to_numeric, errors='coerce')\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['SPANISH_HISPANIC_ORIGIN'].value_counts() #9 means unknown if HISPANIC, 99 FOR RACE MEANS RACE UNKNOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.rename(columns={'CS_SITESPECIFIC_FACTOR_1': \"DEPTH\", 'CS_SITESPECIFIC_FACTOR_2': 'ULCERATION',\n",
    " #      'CS_SITESPECIFIC_FACTOR_7': 'MITOSES', 'CS_SITESPECIFIC_FACTOR_3': 'CS_LYMPH_NODE_METS'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attribs = ['AGE', 'SEX', 'RACE', 'SPANISH_HISPANIC_ORIGIN', 'DEPTH', 'ULCERATION',\n",
    "#           'MITOSES', 'CS_EXTENSION', 'PRIMARY_SITE', 'REGIONAL_NODES_POSITIVE']\n",
    "#attribs = ['AGE', 'SEX',  'DEPTH', 'ULCERATION',\n",
    "  #      'MITOSES', 'CS_EXTENSION', 'PRIMARY_SITE', 'CS_LYMPH_NODE_METS']\n",
    "\n",
    "#attribs = ['AGE', 'SEX', 'DEPTH', 'ULCERATION',\n",
    "#           'MITOSES', 'CS_EXTENSION', 'PRIMARY_SITE', 'TNM_PATH_N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Missing Values\n",
    "For most of our attributes, a missing value is coded as 999. We care remove these entries easily by using .dropna(). However, I need to do additional cleaning for each of our attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['CS_LYMPH_NODE_METS'] = data['CS_LYMPH_NODE_METS'].replace([5, 10],[0, 1])\n",
    "#data = data[data['CS_LYMPH_NODE_METS'] <= 1.5]\n",
    "##data['CS_LYMPH_NODE_METS'].value_counts()\n",
    "#print(data['TNM_PATH_N'].value_counts())\n",
    "#\n",
    "#import numpy as np\n",
    "##data['TNM_PATH_N'] = data['TNM_PATH_N'].replace(['p0', 'pX', 'p1A', 'p2A', 'p1', 'p3', 'p2C', 'p2', 'p1B', 'p2B', '88'], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "#\n",
    "#print(data['TNM_PATH_N'].value_counts())\n",
    "#\n",
    "#print(sum(data['CS_LYMPH_NODE_METS']))\n",
    "#print(sum(data['TNM_PATH_N']))\n",
    "#data['REGIONAL_NODES_POSITIVE'] = data['REGIONAL_NODES_POSITIVE'].replace([98, 99], [0, 0])\n",
    "#data['POSITIVE'] = np.where(data['TNM_PATH_N'] + data['CS_LYMPH_NODE_METS'] + data['REGIONAL_NODES_POSITIVE'] > 0, 1, 0)\n",
    "#print(sum(data['POSITIVE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>ULCERATION</th>\n",
       "      <th>MITOSES</th>\n",
       "      <th>CS_EXTENSION</th>\n",
       "      <th>PRIMARY_SITE</th>\n",
       "      <th>CS_LYMPH_NODE_METS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>860085</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>442</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860086</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>446</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860087</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>446</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860159</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>445</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860170</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>447</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AGE  SEX  DEPTH  ULCERATION  MITOSES  CS_EXTENSION PRIMARY_SITE  \\\n",
       "860085   81    1   89.0         0.0      0.0         300.0          442   \n",
       "860086   51    1   50.0         0.0      0.0         200.0          446   \n",
       "860087   53    1   42.0         0.0      0.0         100.0          446   \n",
       "860159   40    2   34.0         0.0      0.0         100.0          445   \n",
       "860170   34    2   88.0         0.0      1.0         300.0          447   \n",
       "\n",
       "        CS_LYMPH_NODE_METS  \n",
       "860085                 5.0  \n",
       "860086                 5.0  \n",
       "860087                 5.0  \n",
       "860159                 5.0  \n",
       "860170                 5.0  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = data[(data != 999).all(1)]\n",
    "new_df = new_df[(new_df != 1022).all(1)]\n",
    "new_df = new_df[new_df['DEPTH'] != 980]\n",
    "#new_df = new_df[new_df['DEPTH'] <= 400]\n",
    "\n",
    "\n",
    "#new_df = new_df[new_df['SPANISH_HISPANIC_ORIGIN'] != 9]\n",
    "new_df = new_df[new_df['MITOSES'] != 988]\n",
    "\n",
    "#new_df.info()\n",
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Feature Values\n",
    "For some of our features, there have been two coding schema. For example, `ULCERATION` = 1.0 and `ULCERATION` = 10.0 both correspond to the presence of ulceration. Below, we combine our features in order to better represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    203406\n",
       "1.0     38163\n",
       "Name: ULCERATION, dtype: int64"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['ULCERATION'] = new_df['ULCERATION'].replace(10.0, 1.0)\n",
    "new_df['ULCERATION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CS_EXTENSION` attribute has a combination of features for the depth of the extension as well as the Clark Level. For now, I have removed anything that isn't about the Clark Level of the tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.0    84144\n",
      "100.0    58838\n",
      "200.0    46192\n",
      "400.0    12158\n",
      "310.0     9927\n",
      "500.0     9024\n",
      "0.0       4736\n",
      "330.0     3759\n",
      "315.0     3033\n",
      "350.0     1804\n",
      "375.0     1768\n",
      "355.0     1479\n",
      "950.0     1167\n",
      "335.0     1041\n",
      "370.0     1011\n",
      "320.0      475\n",
      "360.0      280\n",
      "380.0      273\n",
      "340.0      251\n",
      "800.0      209\n",
      "Name: CS_EXTENSION, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300.0    84144\n",
       "100.0    58838\n",
       "200.0    46192\n",
       "500.0     9024\n",
       "0.0       4736\n",
       "800.0      209\n",
       "Name: CS_EXTENSION, dtype: int64"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_df['CS_EXTENSION'].value_counts())\n",
    "new_df['CS_EXTENSION'] = new_df['CS_EXTENSION'].replace([\n",
    "    310, 315, 320, 330, 335, 340, 350, 355, 360, 370, 375, 380, 400, 800, 950],\n",
    "    [999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 800, 999])\n",
    "new_df = new_df[(new_df != 999).all(1)]\n",
    "new_df['CS_EXTENSION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df['REGIONAL_NODES_POSITIVE'] = new_df['REGIONAL_NODES_POSITIVE'].replace(98, 0)\n",
    "#new_df['REGIONAL_NODES_POSITIVE'].value_counts()\n",
    "#new_df = new_df[(new_df['REGIONAL_NODES_POSITIVE'] != 99)]\n",
    "\n",
    "#new_df['REGIONAL_NODES_POSITIVE'] = new_df['REGIONAL_NODES_POSITIVE'] > 0 \n",
    "#new_df['REGIONAL_NODES_POSITIVE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    185344\n",
       "1.0     12214\n",
       "Name: CS_LYMPH_NODE_METS, dtype: int64"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['CS_LYMPH_NODE_METS'] = new_df['CS_LYMPH_NODE_METS'].replace([5, 10],[0, 1])\n",
    "new_df = new_df[new_df['CS_LYMPH_NODE_METS'] <= 1.5]\n",
    "new_df['CS_LYMPH_NODE_METS'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df['TNM_PATH_N'].value_counts()\n",
    "#new_df = new_df[new_df['TNM_PATH_N'] != '88']\n",
    "#new_df = new_df[new_df['TNM_PATH_N'] != 'pX']\n",
    "\n",
    "#new_df['TNM_PATH_N'].value_counts()\n",
    "#new_df['TNM_PATH_N'] = new_df['TNM_PATH_N'].apply(lambda x: 0 if x == 'pX' else x)\n",
    "#new_df['TNM_PATH_N'] = new_df['TNM_PATH_N'].apply(lambda x: 0 if x == 'p0' else 1)\n",
    "#new_df['TNM_PATH_N'] = new_df['TNM_PATH_N'].apply(lambda x: x if x == 0 or x == 1 else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df['TNM_PATH_N'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `RACE` attribute, there are several demographics that we only have a single representative; for simplicity, we split these demographics into white, black, and other and hispanic or not hispanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#new_df['RACE'].value_counts()\n",
    "#a = np.array(new_df['RACE'].values.tolist())\n",
    "#new_df['RACE'] = np.where(a > 2, 3, a).tolist()\n",
    "#new_df['RACE'].value_counts()#new_df['RACE'] = new_df['RACE'].apply(lambda x: [y if y <= 1 else 2 for y in x])\n",
    "##new_df['RACE']\n",
    "#\n",
    "#a = np.array(new_df['SPANISH_HISPANIC_ORIGIN'].values.tolist())\n",
    "#new_df['SPANISH_HISPANIC_ORIGIN'] = np.where(a > 1, 1, a).tolist()\n",
    "#new_df['SPANISH_HISPANIC_ORIGIN'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other feature used is the `PRIMARY_SITE`, which codes where the melanoma is located. The features are fairly specific, so for now I coarse grain this into three categories: Head and neck, Trunk, or Extremity.\n",
    "\n",
    "Current labels are\n",
    "\n",
    "- `c440`: Skin of lip -> Head or Neck\n",
    "- `c441`: Eyelid -> Head or Neck\n",
    "- `c442`: External ear -> Head or Neck\n",
    "- `c443`: Skin of other and unspecified parts of face -> Head or Neck\n",
    "- `c444`: Skin of scalp and neck -> Head or Neck\n",
    "- `c445`: Skin of trunk -> Trunk\n",
    "- `c446`: Skin of upper limb and shoulder -> Extremity\n",
    "- `c447`: Skin of lower limb and hip -> Extremity\n",
    "- `c448`: Overlapping lesion of skin -> ?\n",
    "- `c449`: Skin, NOS -> ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df['DEPTH_3'] = new_df['DEPTH'] ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 196806 entries, 4 to 860170\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   AGE                 196806 non-null  int64  \n",
      " 1   SEX                 196806 non-null  int64  \n",
      " 2   DEPTH               196806 non-null  float64\n",
      " 3   ULCERATION          196806 non-null  float64\n",
      " 4   MITOSES             196806 non-null  float64\n",
      " 5   CS_EXTENSION        196806 non-null  float64\n",
      " 6   PRIMARY_SITE        196806 non-null  int64  \n",
      " 7   CS_LYMPH_NODE_METS  196806 non-null  float64\n",
      "dtypes: float64(5), int64(3)\n",
      "memory usage: 13.5 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "new_df['PRIMARY_SITE'].value_counts()\n",
    "new_df['PRIMARY_SITE'] = new_df['PRIMARY_SITE'].replace([\n",
    "    'C440', 'C441', 'C442', 'C443', 'C444', 'C445', 'C446', 'C447', 'C448', 'C449'],\n",
    "    #[0,1,2,3,4,5, 6,7,999,999])\n",
    "    [0,0,0,0,0,1, 2,2,999,999])\n",
    "new_df['PRIMARY_SITE'] = new_df['PRIMARY_SITE'].replace([\n",
    "    '440', '441', '442', '443', '444', '445', '446', '447', '448', '449'],\n",
    "    [0,0,0,0,0,1,2,2,999,999])\n",
    "\n",
    "new_df = new_df[(new_df != 999).all(1)]\n",
    "\n",
    "new_df['PRIMARY_SITE'].value_counts()\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196806\n",
      "0.062025843513362836\n",
      "0.06142980539606727\n",
      "0.06147983486821213\n"
     ]
    }
   ],
   "source": [
    "#attribs = ['AGE', 'SEX', 'RACE', 'SPANISH_HISPANIC_ORIGIN', 'DEPTH', 'DEPTH_3', 'ULCERATION',\n",
    "#           'MITOSES', 'CS_EXTENSION', 'PRIMARY_SITE']\n",
    "\n",
    "attribs = ['AGE', 'SEX', 'DEPTH', 'ULCERATION',\n",
    "            'MITOSES', 'CS_EXTENSION', 'PRIMARY_SITE']\n",
    "\n",
    "X = new_df[attribs]\n",
    "y = new_df['CS_LYMPH_NODE_METS']\n",
    "#y = new_df['REGIONAL_NODES_POSITIVE']\n",
    "print(len(X))\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=20)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.10, random_state=20)\n",
    "\n",
    "\n",
    "#stratifying\n",
    "tmp = X_cv\n",
    "tmp['TARGET'] = y\n",
    "c1 = tmp['DEPTH'] <= 100\n",
    "c2 = tmp['ULCERATION'] == 0\n",
    "c3 = tmp['MITOSES'] == 0\n",
    "c4 = (tmp['DEPTH'] > 100) & (tmp['DEPTH'] <= 200)\n",
    "c5 = (tmp['DEPTH'] > 200) & (tmp['DEPTH'] <= 400)\n",
    "\n",
    "T1a = tmp[c1 & c2 & c3]\n",
    "y1a = T1a['TARGET']\n",
    "T1a = T1a.drop(columns=[\"TARGET\"])\n",
    "\n",
    "T1b = tmp[c1 & ~(c2 & c3)]\n",
    "y1b = T1b['TARGET']\n",
    "T1b = T1b.drop(columns=[\"TARGET\"])\n",
    "\n",
    "T2a = tmp[c4 & c2]\n",
    "y2a = T2a['TARGET']\n",
    "T2a = T2a.drop(columns=[\"TARGET\"])\n",
    "\n",
    "T2b = tmp[c4 & ~(c2)]\n",
    "y2b = T2b['TARGET']\n",
    "T2b = T2b.drop(columns=[\"TARGET\"])\n",
    "\n",
    "T3a = tmp[c5 & c2]\n",
    "y3a = T3a['TARGET']\n",
    "T3a = T3a.drop(columns=[\"TARGET\"])\n",
    "\n",
    "T3b = tmp[c5 & ~(c2)]\n",
    "y3b = T3b['TARGET']\n",
    "T3b = T3b.drop(columns=[\"TARGET\"])\n",
    "#T1 = tmp[tmp['DEPTH' <= 100]]\n",
    "\n",
    "\n",
    "print(sum(y_train)/len(X_train))\n",
    "print(sum(y_test)/len(X_test))\n",
    "print(sum(y_cv)/len(X_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Attributes\n",
    "\n",
    "We have a few <b>numerical attributes</b>\n",
    "- AGE\n",
    "- DEPTH\n",
    "- MITOSES\n",
    "- CS_EXTENSION\n",
    "\n",
    "\n",
    "<b>Categorical Attributes</b>\n",
    "- SEX\n",
    "- RACE\n",
    "- SPANISH_HISPANIC_ORIGIN\n",
    "- ULCERATION\n",
    "- PRIMARY_SITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names]\n",
    "    \n",
    "    \n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X,y=None):\n",
    "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X], index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent_)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"select_numeric\", DataFrameSelector([\"AGE\",\"DEPTH\", \"CS_EXTENSION\", \"MITOSES\"])),\n",
    "    ('poly', PolynomialFeatures(degree=3)),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "     ])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"select_cat\", DataFrameSelector(['SEX','ULCERATION', 'PRIMARY_SITE'])),\n",
    "    (\"imputer\", MostFrequentImputer()),\n",
    "    (\"cat_encoder\", OneHotEncoder(sparse=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CS_LYMPH_NODE_METS    1.000000\n",
       "DEPTH                 0.258939\n",
       "CS_EXTENSION          0.213738\n",
       "ULCERATION            0.195658\n",
       "SEX                  -0.015521\n",
       "MITOSES              -0.056798\n",
       "AGE                  -0.063927\n",
       "Name: CS_LYMPH_NODE_METS, dtype: float64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = new_df.corr()\n",
    "corr_matrix[\"CS_LYMPH_NODE_METS\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142241\n"
     ]
    }
   ],
   "source": [
    "train_num = num_pipeline.fit_transform(X_train)\n",
    "print(len(train_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_pipeline.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data\n",
    "\n",
    "Now, I want to take out the numerical attributes and the categorical attributes and format the data according to the pipeline above. In addition, I split the data again into a cross-validation set in order to better tune the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline)\n",
    "])\n",
    "\n",
    "x_train = preprocess_pipeline.fit_transform(X_train)\n",
    "#print(X_train.head())\n",
    "\n",
    "#print(len(x_train[0]))\n",
    "#sc = MinMaxScaler()\n",
    "#x_train = sc.fit_transform(x_train)\n",
    "x_cv = preprocess_pipeline.transform(X_cv)\n",
    "x_test = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "x_t1a = preprocess_pipeline.transform(T1a)\n",
    "x_t1b = preprocess_pipeline.transform(T1b)\n",
    "\n",
    "x_t2a = preprocess_pipeline.transform(T2a)\n",
    "x_t2b = preprocess_pipeline.transform(T2b)\n",
    "\n",
    "x_t3a = preprocess_pipeline.transform(T3a)\n",
    "x_t3b = preprocess_pipeline.transform(T3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.e+00 1.e+06 0.e+00 0.e+00 0.e+00 0.e+00 1.e+00 1.e+00 0.e+00 0.e+00\n",
      " 0.e+00 1.e+00]\n"
     ]
    }
   ],
   "source": [
    "#print(x_train[0][30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for Estimating\n",
    "\n",
    "Our baseline for estimating is to predict the majority class, i.e. that everyone doesn't need the SLNB. This gives us a point of comparison for the accuracy of our method: we must perform better than this in a meaningful way for our new model to be performing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class BaseLine(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self,X):\n",
    "        return np.zeros((len(X),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9384495266299094"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "base_line_clf = BaseLine()\n",
    "base_scores = cross_val_score(base_line_clf,x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "base_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", C = 0.001)\n",
    "log_clf.fit(x_train,y_train)\n",
    "#log_scores = cross_val_score(log_clf, x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09162379409875863\n",
      "The confusion matrix is [[  4215 128695]\n",
      " [    21   8768]]\n",
      "Feature: 0, Score: 0.00000\n",
      "Feature: 1, Score: -0.62358\n",
      "Feature: 2, Score: 0.51117\n",
      "Feature: 3, Score: 0.30839\n",
      "Feature: 4, Score: -0.14793\n",
      "Feature: 5, Score: -0.51273\n",
      "Feature: 6, Score: 0.27305\n",
      "Feature: 7, Score: 0.13854\n",
      "Feature: 8, Score: -0.08878\n",
      "Feature: 9, Score: 0.19894\n",
      "Feature: 10, Score: 0.23189\n",
      "Feature: 11, Score: 0.03057\n",
      "Feature: 12, Score: 0.21607\n",
      "Feature: 13, Score: 0.00247\n",
      "Feature: 14, Score: -0.15760\n",
      "Feature: 15, Score: -0.43424\n",
      "Feature: 16, Score: 0.14942\n",
      "Feature: 17, Score: 0.05520\n",
      "Feature: 18, Score: -0.05762\n",
      "Feature: 19, Score: 0.09745\n",
      "Feature: 20, Score: 0.14433\n",
      "Feature: 21, Score: 0.02140\n",
      "Feature: 22, Score: 0.11091\n",
      "Feature: 23, Score: 0.00301\n",
      "Feature: 24, Score: -0.09416\n",
      "Feature: 25, Score: 0.06894\n",
      "Feature: 26, Score: 0.08590\n",
      "Feature: 27, Score: 0.01270\n",
      "Feature: 28, Score: 0.09070\n",
      "Feature: 29, Score: 0.01590\n",
      "Feature: 30, Score: 0.02705\n",
      "Feature: 31, Score: 0.10577\n",
      "Feature: 32, Score: 0.01048\n",
      "Feature: 33, Score: -0.00186\n",
      "Feature: 34, Score: -0.15641\n",
      "Feature: 35, Score: -0.33629\n",
      "Feature: 36, Score: -0.44229\n",
      "Feature: 37, Score: -0.93433\n",
      "Feature: 38, Score: 0.15575\n",
      "Feature: 39, Score: -0.39013\n",
      "Feature: 40, Score: -0.15817\n",
      "Feature: 41, Score: -0.23029\n",
      "[[  4215 128695]\n",
      " [    21   8768]]\n",
      "The precision is 0.06378443653928693, the recall is 0.997610649675731, and the f1 score is 0.11990263381013594\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.02\n",
    "y_pred = np.where(log_clf.predict_proba(x_train)[:,1] > THRESHOLD, 1, 0)\n",
    "log_score = accuracy_score(y_train,y_pred)\n",
    "print(log_score)\n",
    "#log_scores.mean()\n",
    "print(f\"The confusion matrix is {confusion_matrix(y_train,y_pred)}\")\n",
    "importance = log_clf.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "#y_pred = y_pred == 2\n",
    "#y_simple = y_train == 2\n",
    "log_f1 = f1_score(y_train,y_pred)\n",
    "log_recall = recall_score(y_train, y_pred)\n",
    "log_precision = precision_score(y_train,y_pred)\n",
    "print(f\"The precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5, 23, 17, 13, 32, 29, 16, 21, 40, 30, 41, 18, 38, 35, 39, 11,\n",
       "       36,  8, 12, 33, 37, 24, 15,  1, 14, 34, 26, 22, 27,  7, 19, 31,  4,\n",
       "        6, 28, 25, 10, 20,  3,  9,  2])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(importance).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196806\n",
      "0.3149571292473801\n",
      "The precision is 0.08207049208241103, the recall is 0.9958677685950413, and the f1 score is 0.15164385716532955\n",
      "The confusion matrix is \n",
      " [[ 3995 10782]\n",
      " [    4   964]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "THRESHOLD = 0.005\n",
    "y_pred = np.where(log_clf.predict_proba(x_cv)[:,1] > THRESHOLD, 1, 0)\n",
    "print(len(X))\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "log_score = accuracy_score(y_cv, y_pred)\n",
    "print(log_score)\n",
    "#print(y_test)\n",
    "log_f1 = f1_score(y_cv,y_pred)\n",
    "log_recall = recall_score(y_cv, y_pred)\n",
    "log_precision = precision_score(y_cv,y_pred)\n",
    "print(f\"The precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "print(f\"The confusion matrix is \\n {confusion_matrix(y_cv,y_pred)}\")\n",
    "\n",
    "\n",
    "y_proba = log_clf.predict_proba(x_cv)\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "#precision, recall, thresholds = precision_recall_curve(y_cv, y_proba)\n",
    "\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_cv, y_pred)\n",
    "\n",
    "disp = plot_precision_recall_curve(log_clf, x_cv, y_cv)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))\n",
    "\n",
    "\n",
    "\n",
    "misclassified = np.where(y_cv != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9671512176703795\n",
      "The T1a precision is 0.017857142857142856, the recall is 0.25, and the f1 score is 0.03333333333333333\n",
      "The T1a confusion matrix is \n",
      " [[5120  165]\n",
      " [   9    3]]\n",
      "\n",
      "\n",
      "\n",
      "0.8500956836062088\n",
      "The T1b precision is 0.08513708513708514, the recall is 0.45384615384615384, and the f1 score is 0.14337788578371813\n",
      "The T1b confusion matrix is \n",
      " [[3939  634]\n",
      " [  71   59]]\n",
      "\n",
      "\n",
      "\n",
      "0.4076923076923077\n",
      "The T2a precision is 0.12316176470588236, the recall is 0.8626609442060086, and the f1 score is 0.2155495978552279\n",
      "The T2a confusion matrix is \n",
      " [[ 806 1431]\n",
      " [  32  201]]\n",
      "\n",
      "\n",
      "\n",
      "0.2621184919210054\n",
      "The T2b precision is 0.15145228215767634, the recall is 0.9733333333333334, and the f1 score is 0.2621184919210054\n",
      "The T2b confusion matrix is \n",
      " [[ 73 409]\n",
      " [  2  73]]\n",
      "\n",
      "\n",
      "\n",
      "0.21793534932221065\n",
      "The T3a precision is 0.1668520578420467, the recall is 0.9933774834437086, and the f1 score is 0.2857142857142857\n",
      "The T3a confusion matrix is \n",
      " [[ 59 749]\n",
      " [  1 150]]\n",
      "\n",
      "\n",
      "\n",
      "0.199203187250996\n",
      "The T3b precision is 0.19168900804289543, the recall is 1.0, and the f1 score is 0.32170978627671537\n",
      "The T3b confusion matrix is \n",
      " [[  7 603]\n",
      " [  0 143]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.05\n",
    "y_pred = np.where(log_clf.predict_proba(x_t1a)[:,1] > THRESHOLD, 1, 0)\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "log_score = accuracy_score(y1a, y_pred)\n",
    "print(log_score)\n",
    "#print(y_test)\n",
    "log_f1 = f1_score(y1a,y_pred)\n",
    "log_recall = recall_score(y1a, y_pred)\n",
    "log_precision = precision_score(y1a,y_pred)\n",
    "print(f\"The T1a precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "print(f\"The T1a confusion matrix is \\n {confusion_matrix(y1a,y_pred)}\\n\\n\\n\")\n",
    "\n",
    "y_pred = np.where(log_clf.predict_proba(x_t1b)[:,1] > THRESHOLD, 1, 0)\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "log_score = accuracy_score(y1b, y_pred)\n",
    "print(log_score)\n",
    "#print(y_test)\n",
    "log_f1 = f1_score(y1b,y_pred)\n",
    "log_recall = recall_score(y1b, y_pred)\n",
    "log_precision = precision_score(y1b,y_pred)\n",
    "print(f\"The T1b precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "print(f\"The T1b confusion matrix is \\n {confusion_matrix(y1b,y_pred)}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "y_pred = np.where(log_clf.predict_proba(x_t2a)[:,1] > THRESHOLD, 1, 0)\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "log_score = accuracy_score(y2a, y_pred)\n",
    "print(log_score)\n",
    "#print(y_test)\n",
    "log_f1 = f1_score(y2a,y_pred)\n",
    "log_recall = recall_score(y2a, y_pred)\n",
    "log_precision = precision_score(y2a,y_pred)\n",
    "print(f\"The T2a precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "print(f\"The T2a confusion matrix is \\n {confusion_matrix(y2a,y_pred)}\\n\\n\\n\")\n",
    "\n",
    "y_pred = np.where(log_clf.predict_proba(x_t2b)[:,1] > THRESHOLD, 1, 0)\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "log_score = accuracy_score(y2b, y_pred)\n",
    "print(log_score)\n",
    "#print(y_test)\n",
    "log_f1 = f1_score(y2b,y_pred)\n",
    "log_recall = recall_score(y2b, y_pred)\n",
    "log_precision = precision_score(y2b,y_pred)\n",
    "print(f\"The T2b precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "print(f\"The T2b confusion matrix is \\n {confusion_matrix(y2b,y_pred)}\\n\\n\\n\")\n",
    "\n",
    "y_pred = np.where(log_clf.predict_proba(x_t3a)[:,1] > THRESHOLD, 1, 0)\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "log_score = accuracy_score(y3a, y_pred)\n",
    "print(log_score)\n",
    "#print(y_test)\n",
    "log_f1 = f1_score(y3a,y_pred)\n",
    "log_recall = recall_score(y3a, y_pred)\n",
    "log_precision = precision_score(y3a,y_pred)\n",
    "print(f\"The T3a precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "print(f\"The T3a confusion matrix is \\n {confusion_matrix(y3a,y_pred)}\\n\\n\\n\")\n",
    "\n",
    "y_pred = np.where(log_clf.predict_proba(x_t3b)[:,1] > THRESHOLD, 1, 0)\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "log_score = accuracy_score(y3b, y_pred)\n",
    "print(log_score)\n",
    "#print(y_test)\n",
    "log_f1 = f1_score(y3b,y_pred)\n",
    "log_recall = recall_score(y3b, y_pred)\n",
    "log_precision = precision_score(y3b,y_pred)\n",
    "print(f\"The T3b precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "print(f\"The T3b confusion matrix is \\n {confusion_matrix(y3b,y_pred)}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5554"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_miss = x_cv[misclassified]\n",
    "y_proba = log_clf.predict_proba(x_miss)\n",
    "\n",
    "super_misclassified = np.where(y_proba[:][:,1] < 0.10)\n",
    "\n",
    "x_features = x_miss[super_misclassified]\n",
    "\n",
    "len(x_features)\n",
    "\n",
    "#\n",
    "#import seaborn as sns\n",
    "##y_proba[:][:,0]\n",
    "#sns.scatterplot(x=y_proba[:][:,0], y=y_proba[:][:,1])\n",
    "\n",
    "#min(y_proba[:][:,1])\n",
    "#min(y_proba[:][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15805, 745]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-303-621673b31040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15805, 745]"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_cv, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(x_train, y_train).predict(x_train)\n",
    "gnb_score = accuracy_score(y_train, y_pred)\n",
    "print(gnb_score)\n",
    "#print(y_test)\n",
    "gnb_f1 = f1_score(y_train,y_pred)\n",
    "gnb_recall = recall_score(y_train, y_pred)\n",
    "gnb_precision = precision_score(y_train,y_pred)\n",
    "print(f\"The precision is {gnb_precision}, the recall is {gnb_recall}, and the f1 score is {gnb_f1}\")\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "y_pred = gnb.fit(x_train, y_train).predict(x_cv)\n",
    "gnb_score = accuracy_score(y_cv, y_pred)\n",
    "print(gnb_score)\n",
    "#print(y_test)\n",
    "gnb_f1 = f1_score(y_cv,y_pred)\n",
    "gnb_recall = recall_score(y_cv, y_pred)\n",
    "gnb_precision = precision_score(y_cv,y_pred)\n",
    "print(f\"The precision is {gnb_precision}, the recall is {gnb_recall}, and the f1 score is {gnb_f1}\")\n",
    "print(confusion_matrix(y_cv, y_pred))\n",
    "\n",
    "misclassified = np.where(y_cv != y_pred)\n",
    "x_miss = x_cv[misclassified]\n",
    "y_proba = log_clf.predict_proba(x_miss)\n",
    "\n",
    "import seaborn as sns\n",
    "#y_proba[:][:,0]\n",
    "sns.scatterplot(x=y_proba[:][:,0], y=y_proba[:][:,1])\n",
    "\n",
    "print(min(y_proba[:][:,1]))\n",
    "print(min(y_proba[:][:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prone to Overfitting\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=42, max_depth = 15)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(x_train)\n",
    "tree_score = accuracy_score(y_train, y_pred)\n",
    "print(tree_score)\n",
    "#print(y_test)\n",
    "tree_f1 = f1_score(y_train,y_pred)\n",
    "tree_recall = recall_score(y_train, y_pred)\n",
    "tree_precision = precision_score(y_train,y_pred)\n",
    "print(f\"The precision is {tree_precision}, the recall is {tree_recall}, and the f1 score is {tree_f1}\")\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "y_pred = tree.predict(x_cv)\n",
    "tree_score = accuracy_score(y_cv, y_pred)\n",
    "print(tree_score)\n",
    "#print(y_test)\n",
    "tree_f1 = f1_score(y_cv,y_pred)\n",
    "tree_recall = recall_score(y_cv, y_pred)\n",
    "tree_precision = precision_score(y_cv,y_pred)\n",
    "print(f\"The precision is {tree_precision}, the recall is {tree_recall}, and the f1 score is {tree_f1}\")\n",
    "print(confusion_matrix(y_cv, y_pred))\n",
    "\n",
    "misclassified = np.where(y_cv != y_pred)\n",
    "x_miss = x_cv[misclassified]\n",
    "y_proba = log_clf.predict_proba(x_miss)\n",
    "\n",
    "import seaborn as sns\n",
    "#y_proba[:][:,0]\n",
    "sns.scatterplot(x=y_proba[:][:,0], y=y_proba[:][:,1])\n",
    "\n",
    "print(min(y_proba[:][:,1]))\n",
    "print(min(y_proba[:][:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "depth = [10, 20, 30]\n",
    "\n",
    "for d in depth:\n",
    "\n",
    "    forest = RandomForestClassifier(max_depth = d,random_state=42)\n",
    "    forest.fit(x_train,y_train)\n",
    "    y_pred = forest.predict(x_train)\n",
    "    forest_score = accuracy_score(y_train, y_pred)\n",
    "    print(tree_score)\n",
    "    #print(y_test)\n",
    "    forest_f1 = f1_score(y_train,y_pred)\n",
    "    forest_recall = recall_score(y_train, y_pred)\n",
    "    forest_precision = precision_score(y_train,y_pred)\n",
    "    print(f\"The precision is {forest_precision}, the recall is {forest_recall}, and the f1 score is {forest_f1} for depth {d}\")\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "    \n",
    "    y_pred = forest.predict(x_cv)\n",
    "    forest_score = accuracy_score(y_cv, y_pred)\n",
    "    print(forest_score)\n",
    "    #print(y_test)\n",
    "    forest_f1 = f1_score(y_cv,y_pred)\n",
    "    forest_recall = recall_score(y_cv, y_pred)\n",
    "    forest_precision = precision_score(y_cv,y_pred)\n",
    "    print(f\"The precision is {forest_precision}, the recall is {forest_recall}, and the f1 score is {forest_f1} for depth {d}\")\n",
    "    print(confusion_matrix(y_cv, y_pred))\n",
    "    misclassified = np.where(y_cv != y_pred)\n",
    "    x_miss = x_cv[misclassified]\n",
    "    y_proba = log_clf.predict_proba(x_miss)\n",
    "\n",
    "    import seaborn as sns\n",
    "#y_proba[:][:,0]\n",
    "    sns.scatterplot(x=y_proba[:][:,0], y=y_proba[:][:,1])\n",
    "\n",
    "    print(min(y_proba[:][:,1]))\n",
    "    print(min(y_proba[:][:,0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf # Note that tf is the standard way to import tensorflow, much like pd is to pandas\n",
    "# from tensorflow import keras\n",
    "\n",
    "# # The code to build the neural network architecture. \n",
    "\n",
    "# # We will use the Keras sequential API to build the neural network one layer at a time\n",
    "# NN = keras.models.Sequential()\n",
    "\n",
    "# # Next add an input layer\n",
    "# # '.Flatten' tells keras to reshape the data with this layer and not transform the inputs\n",
    "# # We specified the input shape as [28,28] since that is the dimensions of our input arrays\n",
    "# #print(len(x_train), len(x_train[0]))\n",
    "# NN.add(keras.layers.Flatten(input_shape=(len(x_train[0]),)))\n",
    "\n",
    "# # Make the hidden layers\n",
    "# # The '.Dense' part just tells keras/tensorflow that this is just a regular densely connected layer.\n",
    "# NN.add(keras.layers.Dense(10, activation='relu'))\n",
    "# NN.add(keras.layers.Dense(5, activation='relu'))\n",
    "\n",
    "# # Lastly add the output layer\n",
    "# # Remember to specify the softmax activation function\n",
    "# NN.add(keras.layers.Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights, biases = NN.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from keras.optimizers import SGD\n",
    "# opt = keras.optimizers.SGD(learning_rate=0.1)\n",
    "#  NN.compile(loss = 'sparse_categorical_crossentropy', # This is explained much more below\n",
    "#           optimizer = opt, # sgd stands for stochastic gradient descent\n",
    "#           metrics = ['accuracy']) # These are other metrics you would like to calculate after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = NN.fit(x_train, y_train, epochs = 30,\n",
    "#                  validation_data = (x_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim(0,1)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling the classes + single data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df[attribs]\n",
    "y = new_df['CS_LYMPH_NODE_METS']\n",
    "healthy_idx = new_df[new_df['CS_LYMPH_NODE_METS'] == 0.0].index\n",
    "#print(len(healthy_idx))\n",
    "#print(len(y))\n",
    "random_idx = np.random.choice(healthy_idx, 10000, replace=False)\n",
    "##print(len(random_idx))\n",
    "\n",
    "sick_idx = new_df[new_df['CS_LYMPH_NODE_METS'] == 1.0].index\n",
    "all_idx = np.concatenate((random_idx, sick_idx), axis = None)\n",
    "\n",
    "X = X.loc[all_idx]\n",
    "y = y.loc[all_idx]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=20)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.10, random_state=20)\n",
    "\n",
    "x_train = preprocess_pipeline.fit_transform(X_train)\n",
    "x_train\n",
    "x_cv = preprocess_pipeline.transform(X_cv)\n",
    "x_test = preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\")\n",
    "log_clf.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "log_scores = cross_val_score(log_clf, x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "print(min(log_scores))\n",
    "\n",
    "importance = log_clf.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = [0.1, 0.15, 0.2]\n",
    "for thresh in THRESHOLD:\n",
    "    print(f\"THRESHOLD is {thresh}\")\n",
    "    y_pred = np.where(log_clf.predict_proba(x_train)[:,1] > thresh, 1, 0)\n",
    "    log_score = accuracy_score(y_train,y_pred)\n",
    "    print(log_score)\n",
    "    log_scores.mean()\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "#y_pred = y_pred == 2\n",
    "#y_simple = y_train == 2\n",
    "    log_f1 = f1_score(y_train,y_pred)\n",
    "    log_recall = recall_score(y_train, y_pred)\n",
    "    log_precision = precision_score(y_train,y_pred)\n",
    "    print(f\"The precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = preprocess_pipeline.transform(X_test) #running our train_set through the pipeline\n",
    "#x_test = sc.transform (x_test)\n",
    "for thresh in THRESHOLD:\n",
    "    print(f\"THRESHOLD is {thresh}\")\n",
    "\n",
    "    y_pred = np.where(log_clf.predict_proba(x_cv)[:,1] > thresh, 1, 0)\n",
    "\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "    log_score = accuracy_score(y_cv, y_pred)\n",
    "    print(log_score)\n",
    "#print(y_test)\n",
    "    log_f1 = f1_score(y_cv,y_pred)\n",
    "    log_recall = recall_score(y_cv, y_pred)\n",
    "    log_precision = precision_score(y_cv,y_pred)\n",
    "    print(f\"The precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "    print(f\"The confusion matrix is \\n {confusion_matrix(y_cv,y_pred)}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "#y_proba = log_clf.predict_proba(x_cv)\n",
    "#from sklearn.metrics import precision_recall_curve\n",
    "##precision, recall, thresholds = precision_recall_curve(y_cv, y_proba)\n",
    "#\n",
    "#from sklearn.metrics import plot_precision_recall_curve\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics import average_precision_score\n",
    "#average_precision = average_precision_score(y_cv, y_pred)\n",
    "#\n",
    "#disp = plot_precision_recall_curve(log_clf, x_cv, y_cv)\n",
    "#disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "#                   'AP={0:0.2f}'.format(average_precision))\n",
    "#\n",
    "#\n",
    "#\n",
    "#misclassified = np.where(y_cv != y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding in more data rofl hahaha this definitely works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mela = pd.read_csv('seer_melanoma.csv')\n",
    "new_mela = new_mela.rename(columns={'CS_SITESPECIFIC_FACTOR_1': \"DEPTH\", 'CS_SITESPECIFIC_FACTOR_2': 'ULCERATION',\n",
    "       'CS_SITESPECIFIC_FACTOR_7': 'MITOSES', 'CS_SITESPECIFIC_FACTOR_3': 'CS_LYMPH_NODE_METS'})\n",
    "attribs = ['AGE', 'SEX', 'DEPTH', 'ULCERATION',\n",
    "        'MITOSES', 'CS_EXTENSION', 'PRIMARY_SITE', 'CS_LYMPH_NODE_METS']\n",
    "\n",
    "new_data = new_mela[attribs].dropna()\n",
    "\n",
    "\n",
    "#### NEW SECTION\n",
    "#new_data['CS_LYMPH_NODE_METS'] = new_data['CS_LYMPH_NODE_METS'].replace([5, 10],[0, 1])\n",
    "#new_data = new_data[new_data['CS_LYMPH_NODE_METS'] <= 1.5]\n",
    "#data['CS_LYMPH_NODE_METS'].value_counts()\n",
    "#import numpy as np\n",
    "#data['TNM_PATH_N'] = data['TNM_PATH_N'].replace(['p0', 'pX', 'p1A', 'p2A', 'p1', 'p3', 'p2C', 'p2', 'p1B', 'p2B', '88'], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "#new_data['REGIONAL_NODES_POSITIVE'] = new_data['REGIONAL_NODES_POSITIVE'].replace([98, 99], [0, 0])\n",
    "#new_data['POSITIVE'] = np.where(new_data['TNM_PATH_N'] + new_data['CS_LYMPH_NODE_METS'] + new_data['REGIONAL_NODES_POSITIVE'] > 0, 1, 0)\n",
    "\n",
    "### END OF NEW SECTION\n",
    "\n",
    "df = new_data[(new_data != 999).all(1)]\n",
    "df = df[df['MITOSES'] != 988]\n",
    "df = df.dropna()\n",
    "df = df[(df != 1022).all(1)]\n",
    "df['ULCERATION'] = df['ULCERATION'].replace(10, 1)\n",
    "\n",
    "df['CS_EXTENSION'].value_counts()\n",
    "df['CS_EXTENSION'] = df['CS_EXTENSION'].replace([\n",
    "    310, 315, 320, 330, 335, 340, 350, 355, 360, 370, 375, 380, 400, 950],\n",
    "    [999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999,  999])\n",
    "df = df[(df != 999).all(1)]\n",
    "df = df[df['AGE'] <= 84]\n",
    "df['CS_LYMPH_NODE_METS'] = df['CS_LYMPH_NODE_METS'].replace([5, 10],[0, 1])\n",
    "df = df[df['CS_LYMPH_NODE_METS'] <= 1.5]\n",
    "df['CS_LYMPH_NODE_METS'].value_counts()\n",
    "print(df['PRIMARY_SITE'].value_counts())\n",
    "df['PRIMARY_SITE'] = df['PRIMARY_SITE'].replace([\n",
    "    440, 441, 442, 443, 444, 445, 446, 447, 448, 449],  [0,0,0,0,0,1, 2,2,999,999])\n",
    "print(df['PRIMARY_SITE'].value_counts())\n",
    "df = df[(df != 999).all(1)]\n",
    "df = df[df['MITOSES'] <= 11]\n",
    "\n",
    "#print(df.describe())\n",
    "#print(new_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attribs = ['AGE', 'SEX', 'RACE', 'SPANISH_HISPANIC_ORIGIN', 'DEPTH', 'DEPTH_3', 'ULCERATION',\n",
    "#           'MITOSES', 'CS_EXTENSION', 'PRIMARY_SITE']\n",
    "\n",
    "attribs = ['AGE', 'SEX', 'DEPTH', 'ULCERATION',\n",
    "            'MITOSES', 'CS_EXTENSION', 'PRIMARY_SITE']\n",
    "#print(new_df.describe())\n",
    "concat_data = pd.concat([new_df, df])\n",
    "print(concat_data.describe())\n",
    "#X = new_df[attribs]\n",
    "X = concat_data[attribs]\n",
    "#y = new_df['CS_LYMPH_NODE_METS']\n",
    "y = concat_data['CS_LYMPH_NODE_METS']\n",
    "#y = new_df['REGIONAL_NODES_POSITIVE']\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=20)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.10, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocess_pipeline.fit_transform(X_train)\n",
    "x_train\n",
    "x_cv = preprocess_pipeline.transform(X_cv)\n",
    "x_test = preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", class_weight=\"balanced\")\n",
    "log_clf.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "log_scores = cross_val_score(log_clf, x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "print(min(log_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.03\n",
    "y_pred = np.where(log_clf.predict_proba(x_train)[:,1] > THRESHOLD, 1, 0)\n",
    "log_score = accuracy_score(y_train,y_pred)\n",
    "print(log_score)\n",
    "log_scores.mean()\n",
    "print(f\"The confusion matrix is {confusion_matrix(y_train,y_pred)}\")\n",
    "importance = log_clf.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "#y_pred = y_pred == 2\n",
    "#y_simple = y_train == 2\n",
    "log_f1 = f1_score(y_train,y_pred)\n",
    "log_recall = recall_score(y_train, y_pred)\n",
    "log_precision = precision_score(y_train,y_pred)\n",
    "print(f\"The precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = preprocess_pipeline.transform(X_test) #running our train_set through the pipeline\n",
    "#x_test = sc.transform (x_test)\n",
    "#THRESHOLD = 0.001\n",
    "y_pred = np.where(log_clf.predict_proba(x_cv)[:,1] > THRESHOLD, 1, 0)\n",
    "\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "log_score = accuracy_score(y_cv, y_pred)\n",
    "print(log_score)\n",
    "#print(y_test)\n",
    "log_f1 = f1_score(y_cv,y_pred)\n",
    "log_recall = recall_score(y_cv, y_pred)\n",
    "log_precision = precision_score(y_cv,y_pred)\n",
    "print(f\"The precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "print(f\"The confusion matrix is \\n {confusion_matrix(y_cv,y_pred)}\")\n",
    "\n",
    "\n",
    "y_proba = log_clf.predict_proba(x_cv)\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "#precision, recall, thresholds = precision_recall_curve(y_cv, y_proba)\n",
    "\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_cv, y_pred)\n",
    "\n",
    "disp = plot_precision_recall_curve(log_clf, x_cv, y_cv)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))\n",
    "\n",
    "\n",
    "\n",
    "misclassified = np.where(y_cv != y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling + 2 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = concat_data[attribs]\n",
    "y = concat_data['CS_LYMPH_NODE_METS']\n",
    "healthy_idx = concat_data[concat_data['CS_LYMPH_NODE_METS'] == 0.0].index\n",
    "#print(len(healthy_idx))\n",
    "#print(len(y))\n",
    "random_idx = np.random.choice(healthy_idx, 12000, replace=False)\n",
    "##print(len(random_idx))\n",
    "\n",
    "sick_idx = concat_data[concat_data['CS_LYMPH_NODE_METS'] == 1.0].index\n",
    "all_idx = np.concatenate((random_idx, sick_idx), axis = None)\n",
    "\n",
    "X = X.loc[all_idx]\n",
    "y = y.loc[all_idx]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=20)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.10, random_state=20)\n",
    "\n",
    "x_train = preprocess_pipeline.fit_transform(X_train)\n",
    "x_train\n",
    "x_cv = preprocess_pipeline.transform(X_cv)\n",
    "x_test = preprocess_pipeline.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\")\n",
    "log_clf.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "log_scores = cross_val_score(log_clf, x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "print(min(log_scores))\n",
    "\n",
    "\n",
    "importance = log_clf.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "for thresh in THRESHOLD:\n",
    "    print(f\"THRESHOLD is {thresh}\")\n",
    "    y_pred = np.where(log_clf.predict_proba(x_train)[:,1] > thresh, 1, 0)\n",
    "    log_score = accuracy_score(y_train,y_pred)\n",
    "    print(log_score)\n",
    "    log_scores.mean()\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "#y_pred = y_pred == 2\n",
    "#y_simple = y_train == 2\n",
    "    log_f1 = f1_score(y_train,y_pred)\n",
    "    log_recall = recall_score(y_train, y_pred)\n",
    "    log_precision = precision_score(y_train,y_pred)\n",
    "    print(f\"The precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = preprocess_pipeline.transform(X_test) #running our train_set through the pipeline\n",
    "#x_test = sc.transform (x_test)\n",
    "for thresh in THRESHOLD:\n",
    "    print(f\"THRESHOLD is {thresh}\")\n",
    "\n",
    "    y_pred = np.where(log_clf.predict_proba(x_cv)[:,1] > thresh, 1, 0)\n",
    "\n",
    "#y_cv_test = y_cv == 2\n",
    "#print(sum(y_cv_test))\n",
    "#y_pred = y_pred == 2\n",
    "#print(sum(y_pred == 2))\n",
    "    log_score = accuracy_score(y_cv, y_pred)\n",
    "    print(log_score)\n",
    "#print(y_test)\n",
    "    log_f1 = f1_score(y_cv,y_pred)\n",
    "    log_recall = recall_score(y_cv, y_pred)\n",
    "    log_precision = precision_score(y_cv,y_pred)\n",
    "    print(f\"The precision is {log_precision}, the recall is {log_recall}, and the f1 score is {log_f1}\")\n",
    "    print(f\"The confusion matrix is \\n {confusion_matrix(y_cv,y_pred)}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
